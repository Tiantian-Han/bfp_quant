{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lb/miniconda3/envs/mxbfp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from mx import mx_mapping, finalize_mx_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, device, epochs=10, model_path='best_model_fp32.pth'):\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Model file {model_path} already exists. Skipping training.\")\n",
    "        return model_path\n",
    "\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} done. Loss: {epoch_loss}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing and loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, test_loader, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    accuracy = (100.0 * correct) / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model wrapper class\n",
    "class ModelWrapper:\n",
    "    def __init__(self, model_fp8_path, model_fp4_path, device):\n",
    "        self.model_fp8_path = model_fp8_path\n",
    "        self.model_fp4_path = model_fp4_path\n",
    "        self.device = device\n",
    "        self.current_model = SimpleCNN().to(device)\n",
    "        self.using_fp4 = True\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def eval(self):\n",
    "        self.current_model.eval()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.current_model(x)\n",
    "\n",
    "    def restore_full_precision(self):\n",
    "        self.inject_fp8()\n",
    "        self.current_model.load_state_dict(torch.load(self.model_fp8_path))\n",
    "        self.current_model.to(self.device)\n",
    "        self.using_fp4 = False\n",
    "        print(\"Switched to full precision (FP8).\")\n",
    "\n",
    "    def quantize(self):\n",
    "        self.inject_fp4()\n",
    "        self.current_model.load_state_dict(torch.load(self.model_fp4_path))\n",
    "        self.current_model.to(self.device)\n",
    "        self.using_fp4 = True\n",
    "        print(\"Switched to quantized model (FP4).\")\n",
    "\n",
    "    def inject_fp4(self):\n",
    "        mx_specs_fp4 = {\n",
    "            'w_elem_format': 'fp4_e2m1',\n",
    "            'a_elem_format': 'fp4_e2m1',\n",
    "            'scale_bits': 8,\n",
    "            'block_size': 32,\n",
    "            'custom_cuda': True,\n",
    "            'bfloat': 16,\n",
    "        }\n",
    "        mx_specs_fp4 = finalize_mx_specs(mx_specs_fp4)\n",
    "        mx_mapping.inject_pyt_ops(mx_specs_fp4)\n",
    "\n",
    "    def inject_fp8(self):\n",
    "        mx_specs_fp8 = {\n",
    "            'w_elem_format': 'fp8_e5m2',\n",
    "            'a_elem_format': 'fp8_e5m2',\n",
    "            'scale_bits': 8,\n",
    "            'block_size': 32,\n",
    "            'custom_cuda': True,\n",
    "            'bfloat': 16,\n",
    "        }\n",
    "        mx_specs_fp8 = finalize_mx_specs(mx_specs_fp8)\n",
    "        mx_mapping.inject_pyt_ops(mx_specs_fp8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection and model swapping function\n",
    "def anomaly_detection_swap(model_wrapper, test_loader, device, n):\n",
    "    model_wrapper.to(device)\n",
    "    model_wrapper.eval()\n",
    "\n",
    "    consecutive_errors = 0\n",
    "    consecutive_corrects = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_wrapper(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            for idx in range(images.size(0)):\n",
    "                if predicted[idx] == labels[idx]:\n",
    "                    consecutive_errors = 0\n",
    "                    consecutive_corrects += 1\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    consecutive_corrects = 0\n",
    "                    consecutive_errors += 1\n",
    "\n",
    "                # Check if we need to switch model precision\n",
    "                if model_wrapper.using_fp4 and consecutive_errors > n:\n",
    "                    model_wrapper.restore_full_precision()\n",
    "                    consecutive_errors = 0  # Reset counter after switching\n",
    "                    accuracy = evaluate(model_wrapper.current_model, test_loader, device)\n",
    "                    print(f\"Evaluation after switching to FP8: {accuracy:.4f}%\")\n",
    "                elif not model_wrapper.using_fp4 and consecutive_corrects > n:\n",
    "                    model_wrapper.quantize()\n",
    "                    consecutive_corrects = 0  # Reset counter after switching\n",
    "                    accuracy = evaluate(model_wrapper.current_model, test_loader, device)\n",
    "                    print(f\"Evaluation after switching to FP4: {accuracy:.4f}%\")\n",
    "\n",
    "                total += 1\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Final accuracy: {accuracy:.4f}. Final swap lead to {'FP4' if model_wrapper.using_fp4 else 'FP8'} model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file best_model_fp32.pth already exists. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "# Train the model in FP32 for 10 epochs\n",
    "model_fp32 = SimpleCNN().to(device)\n",
    "best_model_fp32_path = train(model_fp32, train_loader, device, epochs=10, model_path='best_model_fp32.pth')\n",
    "model_fp32.load_state_dict(torch.load(best_model_fp32_path))\n",
    "# Evaluate the trained FP32 model\n",
    "accuracy_fp32 = evaluate(model_fp32, test_loader, device)\n",
    "print(f\"FP32 - Accuracy: {accuracy_fp32:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the trained FP32 model to FP8\n",
    "mx_specs_fp8 = {\n",
    "    'w_elem_format': 'fp8_e5m2',\n",
    "    'a_elem_format': 'fp8_e5m2',\n",
    "    'scale_bits': 8,\n",
    "    'block_size': 32,\n",
    "    'custom_cuda': True,\n",
    "    'bfloat': 16,\n",
    "}\n",
    "mx_specs_fp8 = finalize_mx_specs(mx_specs_fp8)\n",
    "mx_mapping.inject_pyt_ops(mx_specs_fp8)\n",
    "\n",
    "model_fp8 = SimpleCNN().to(device)\n",
    "model_fp8.load_state_dict(torch.load(best_model_fp32_path))\n",
    "torch.save(model_fp8.state_dict(), 'best_model_fp8.pth')\n",
    "best_model_fp8_path = 'best_model_fp8.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the trained FP32 model to FP4\n",
    "mx_specs_fp4 = {\n",
    "    'w_elem_format': 'fp4_e2m1',\n",
    "    'a_elem_format': 'fp4_e2m1',\n",
    "    'scale_bits': 8,\n",
    "    'block_size': 32,\n",
    "    'custom_cuda': True,\n",
    "    'bfloat': 16,\n",
    "}\n",
    "mx_specs_fp4 = finalize_mx_specs(mx_specs_fp4)\n",
    "mx_mapping.inject_pyt_ops(mx_specs_fp4)\n",
    "\n",
    "model_fp4 = SimpleCNN().to(device)\n",
    "model_fp4.load_state_dict(torch.load(best_model_fp32_path))\n",
    "torch.save(model_fp4.state_dict(), 'best_model_fp4.pth')\n",
    "best_model_fp4_path = 'best_model_fp4.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to full precision (FP8).\n",
      "Evaluation after switching to FP8: 70.1500%\n",
      "Switched to quantized model (FP4).\n",
      "Evaluation after switching to FP4: 70.1500%\n",
      "Switched to full precision (FP8).\n",
      "Evaluation after switching to FP8: 70.1500%\n",
      "Switched to quantized model (FP4).\n",
      "Evaluation after switching to FP4: 70.1500%\n",
      "Switched to full precision (FP8).\n",
      "Evaluation after switching to FP8: 70.1500%\n",
      "Switched to quantized model (FP4).\n",
      "Evaluation after switching to FP4: 70.1500%\n",
      "Final accuracy: 69.7700. Final swap lead to FP4 model\n"
     ]
    }
   ],
   "source": [
    "# Create a model wrapper\n",
    "model_wrapper = ModelWrapper(best_model_fp8_path, best_model_fp4_path, device)\n",
    "\n",
    "# Anomaly detection and model swapping\n",
    "anomaly_detection_swap(model_wrapper, test_loader, device, n=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mxbfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
