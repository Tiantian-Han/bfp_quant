{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function handles quantizing a tensor to bfp\n",
    "def fp32_to_bfp(tensor, group_size, mantissa_bits):\n",
    "    flat_tensor = tensor.flatten()\n",
    "    original_size = flat_tensor.size(0)\n",
    "    padded_size = (original_size + group_size - 1) // group_size * group_size\n",
    "    padded_tensor = torch.nn.functional.pad(\n",
    "        flat_tensor, (0, padded_size - original_size)\n",
    "    )\n",
    "    padded_tensor = padded_tensor.view(-1, group_size)\n",
    "    max_exponents = torch.max(padded_tensor.abs().log2().ceil(), dim=1, keepdim=True)[0]\n",
    "    aligned_mantissas = padded_tensor * 2 ** (\n",
    "        max_exponents - padded_tensor.abs().log2().ceil()\n",
    "    )\n",
    "    scale = 2**mantissa_bits\n",
    "    truncated_mantissas = torch.floor(aligned_mantissas * scale) / scale\n",
    "    bfp_values = truncated_mantissas * 2 ** (\n",
    "        -max_exponents + padded_tensor.abs().log2().ceil()\n",
    "    )\n",
    "    bfp_values = bfp_values.view(-1)[:original_size]\n",
    "    return bfp_values.view(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quant the whole model\n",
    "def quantize_model_bfp(model, group_size, mantissa_bits):\n",
    "    for param in model.parameters():\n",
    "        with torch.no_grad():\n",
    "            param.copy_(fp32_to_bfp(param, group_size, mantissa_bits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We have our own Linear layer\n",
    "# class BFPLinear(nn.Linear):\n",
    "#     def __init__(\n",
    "#         self, in_features, out_features, bias=True, group_size=16, mantissa_bits=4\n",
    "#     ):\n",
    "#         super(BFPLinear, self).__init__(in_features, out_features, bias)\n",
    "#         self.group_size = group_size\n",
    "#         self.mantissa_bits = mantissa_bits\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         bfp_weight = fp32_to_bfp(self.weight, self.group_size, self.mantissa_bits)\n",
    "#         if self.bias is not None:\n",
    "#             bfp_bias = fp32_to_bfp(self.bias, self.group_size, self.mantissa_bits)\n",
    "#         else:\n",
    "#             bfp_bias = None\n",
    "#         return nn.functional.linear(input, bfp_weight, bfp_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for modyfing models we create (base, quant, and restored)\n",
    "class BFPModelWrapper(nn.Module):\n",
    "    def __init__(self, model, group_size, mantissa_bits):\n",
    "        super(BFPModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.group_size = group_size\n",
    "        self.mantissa_bits = mantissa_bits\n",
    "        self.full_precision_params = {\n",
    "            name: param.clone().detach() for name, param in model.named_parameters()\n",
    "        }\n",
    "\n",
    "    def quantize(self):\n",
    "        quantize_model_bfp(self.model, self.group_size, self.mantissa_bits)\n",
    "\n",
    "    def restore_full_precision(self):\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.model.named_parameters():\n",
    "                param.copy_(self.full_precision_params[name].detach().clone())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # print every 100 mini-batches\n",
    "                print(\n",
    "                    f\"[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}\"\n",
    "                )\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleNN()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = 16\n",
    "mantissa_bits = 4\n",
    "\n",
    "# Wrap the model with BFPModelWrapper\n",
    "bfp_model = BFPModelWrapper(model, group_size=group_size, mantissa_bits=mantissa_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 100] loss: 0.663\n",
      "[Epoch 1, Batch 200] loss: 0.288\n",
      "[Epoch 1, Batch 300] loss: 0.240\n",
      "[Epoch 1, Batch 400] loss: 0.220\n",
      "[Epoch 1, Batch 500] loss: 0.198\n",
      "[Epoch 1, Batch 600] loss: 0.167\n",
      "[Epoch 1, Batch 700] loss: 0.153\n",
      "[Epoch 1, Batch 800] loss: 0.147\n",
      "[Epoch 1, Batch 900] loss: 0.137\n",
      "[Epoch 2, Batch 100] loss: 0.096\n",
      "[Epoch 2, Batch 200] loss: 0.098\n",
      "[Epoch 2, Batch 300] loss: 0.098\n",
      "[Epoch 2, Batch 400] loss: 0.102\n",
      "[Epoch 2, Batch 500] loss: 0.091\n",
      "[Epoch 2, Batch 600] loss: 0.102\n",
      "[Epoch 2, Batch 700] loss: 0.094\n",
      "[Epoch 2, Batch 800] loss: 0.100\n",
      "[Epoch 2, Batch 900] loss: 0.090\n",
      "[Epoch 3, Batch 100] loss: 0.068\n",
      "[Epoch 3, Batch 200] loss: 0.065\n",
      "[Epoch 3, Batch 300] loss: 0.058\n",
      "[Epoch 3, Batch 400] loss: 0.063\n",
      "[Epoch 3, Batch 500] loss: 0.052\n",
      "[Epoch 3, Batch 600] loss: 0.068\n",
      "[Epoch 3, Batch 700] loss: 0.061\n",
      "[Epoch 3, Batch 800] loss: 0.072\n",
      "[Epoch 3, Batch 900] loss: 0.081\n",
      "[Epoch 4, Batch 100] loss: 0.043\n",
      "[Epoch 4, Batch 200] loss: 0.046\n",
      "[Epoch 4, Batch 300] loss: 0.053\n",
      "[Epoch 4, Batch 400] loss: 0.046\n",
      "[Epoch 4, Batch 500] loss: 0.060\n",
      "[Epoch 4, Batch 600] loss: 0.054\n",
      "[Epoch 4, Batch 700] loss: 0.052\n",
      "[Epoch 4, Batch 800] loss: 0.043\n",
      "[Epoch 4, Batch 900] loss: 0.068\n",
      "[Epoch 5, Batch 100] loss: 0.039\n",
      "[Epoch 5, Batch 200] loss: 0.050\n",
      "[Epoch 5, Batch 300] loss: 0.034\n",
      "[Epoch 5, Batch 400] loss: 0.033\n",
      "[Epoch 5, Batch 500] loss: 0.039\n",
      "[Epoch 5, Batch 600] loss: 0.053\n",
      "[Epoch 5, Batch 700] loss: 0.032\n",
      "[Epoch 5, Batch 800] loss: 0.047\n",
      "[Epoch 5, Batch 900] loss: 0.046\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(bfp_model, train_loader, criterion, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the full precision model: 97.54%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the full precision model\n",
    "full_precision_accuracy = evaluate_model(bfp_model, test_loader)\n",
    "print(f\"Accuracy of the full precision model: {full_precision_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the BFP quantized model: 96.87%\n"
     ]
    }
   ],
   "source": [
    "# Quantize the model and evaluate\n",
    "bfp_model.quantize()\n",
    "bfp_accuracy = evaluate_model(bfp_model, test_loader)\n",
    "print(f\"Accuracy of the BFP quantized model: {bfp_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the restored full precision model: 11.74%\n"
     ]
    }
   ],
   "source": [
    "# Restore the full precision model and evaluate\n",
    "bfp_model.restore_full_precision()\n",
    "restored_full_precision_accuracy = evaluate_model(bfp_model, test_loader)\n",
    "print(\n",
    "    f\"Accuracy of the restored full precision model: {restored_full_precision_accuracy}%\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
